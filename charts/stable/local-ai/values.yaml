image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0@sha256:3473f9694c3899d9e8d3c3c887d52f4e7d46cf643a03d7903f108d097a70611f
ffmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-ffmpeg-core@sha256:452969a49bc770873ca7dfeb7d75e06dac1e6b30e30247ca538a68fd19338cb6
cublasCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-cublas-cuda12-core@sha256:adf2dae4041e3a6388b2acdfbf5a61313f9fabaa086a25ecf1c84350d510bf00
cublasCuda12FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-cublas-cuda12-ffmpeg-core@sha256:3fffd61ba96cf33d831e334400f9338b11897305dd62ce230734637663db40ba
cublasCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-cublas-cuda11-core@sha256:f31b9a7221a0de25f9f27d6d309f6dd3eb2ef060210d9c7cc689fa4c8b32dc98
cublasCuda11FfmpegImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-cublas-cuda11-ffmpeg-core@sha256:df42be09082bc75d6d98b8fe31dcb1605b173f149930fc37511ea49128d41222
allInOneCuda12Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-aio-gpu-nvidia-cuda-12@sha256:7c0f92e7a12593da8bb7dae41d44219913cab35ede90b340b0d4b1e1d5e0eab3
allInOneCuda11Image:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-aio-gpu-nvidia-cuda-11@sha256:5c52b338d391c2683c63fd0e382fc6c22cc1f9305701f00aa8cb9fc4a10eb37e
allInOneCpuImage:
  repository: docker.io/localai/localai
  pullPolicy: IfNotPresent
  tag: v2.16.0-aio-cpu@sha256:e3c8e59b16e12f863a2590c15a21fbe1d45bda92c6a300604a833e9bc46b08ca
securityContext:
  container:
    runAsNonRoot: false
    readOnlyRootFilesystem: false
    runAsUser: 0
    runAsGroup: 0
service:
  main:
    ports:
      main:
        protocol: http
        port: 8080
localai:
  # Specify a build type. Available: cublas, openblas, clblas.
  build_type: "openblas"
  debug: false
  cors: true
  cors_allow_origins: "*"
  galleries: []
  #  - name: model-gallery
  #    url: github:go-skynet/model-gallery/index.yaml
  preload_models: []
  #    url: github:go-skynet/model-gallery/gpt4all-j.yaml
  # UPLOAD_LIMIT
workload:
  main:
    podSpec:
      containers:
        main:
          probes:
            liveness:
              enabled: true
              type: http
              path: /readyz
            readiness:
              enabled: true
              type: http
              path: /readyz
            startup:
              enabled: true
              type: tcp
          imageSelector: image
          env:
            ADDRESS: ":{{ .Values.service.main.ports.main.port }}"
            MODELS_PATH: "{{ .Values.persistence.models.mountPath }}"
            IMAGE_PATH: "{{ .Values.persistence.images.mountPath }}"
            BUILD_TYPE: "{{ .Values.localai.build_type }}"
            # breaks chart if true, keep it false.
            REBUILD: false
            DEBUG: "{{ .Values.localai.debug }}"
            CORS: "{{ .Values.localai.cors }}"
            GALLERIES: "{{ toJson .Values.localai.galleries }}"
            PRELOAD_MODELS: "{{ toJson .Values.localai.preload_models }}"
            CORS_ALLOW_ORIGINS: "{{ .Values.localai.cors_allow_origins }}"
persistence:
  models:
    enabled: true
    mountPath: "/models"
  images:
    enabled: true
    mountPath: "/images"
portal:
  open:
    enabled: false
updated: true
